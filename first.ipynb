{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863d2afd-e3b6-49cf-9a47-bbbd25e8fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13bc3eca-b20e-4cf4-ac4b-496a829dba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17113c67-258b-48ae-9eec-e8d5588312d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"India runs on open source. From small startups to large enterprises, developers across the country rely on open source tools and libraries to build fast, reliable, and cost-effective systems.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cba87ebe-92c0-48c0-86b8-6e59ec60e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03a4ddc8-811c-4295-a970-7d59089a0251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'runs', 'on', 'open', 'source', '.', 'From', 'small', 'startups', 'to', 'large', 'enterprises', ',', 'developers', 'across', 'the', 'country', 'rely', 'on', 'open', 'source', 'tools', 'and', 'libraries', 'to', 'build', 'fast', ',', 'reliable', ',', 'and', 'cost', '-', 'effective', 'systems', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.text for token in doc ]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6af79e41-f27b-431a-b524-3afa5faafc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['india',\n",
       " 'runs',\n",
       " 'on',\n",
       " 'open',\n",
       " 'source',\n",
       " '.',\n",
       " 'from',\n",
       " 'small',\n",
       " 'startups',\n",
       " 'to',\n",
       " 'large',\n",
       " 'enterprises',\n",
       " ',',\n",
       " 'developers',\n",
       " 'across',\n",
       " 'the',\n",
       " 'country',\n",
       " 'rely',\n",
       " 'on',\n",
       " 'open',\n",
       " 'source',\n",
       " 'tools',\n",
       " 'and',\n",
       " 'libraries',\n",
       " 'to',\n",
       " 'build',\n",
       " 'fast',\n",
       " ',',\n",
       " 'reliable',\n",
       " ',',\n",
       " 'and',\n",
       " 'cost',\n",
       " '-',\n",
       " 'effective',\n",
       " 'systems',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lower = [token.text.lower() for token in doc ]\n",
    "tokens_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0db70951-2e56-48e2-9d43-c082989372c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['india',\n",
       " 'run',\n",
       " 'open',\n",
       " 'source',\n",
       " 'small',\n",
       " 'startup',\n",
       " 'large',\n",
       " 'enterprise',\n",
       " 'developer',\n",
       " 'country',\n",
       " 'rely',\n",
       " 'open',\n",
       " 'source',\n",
       " 'tool',\n",
       " 'library',\n",
       " 'build',\n",
       " 'fast',\n",
       " 'reliable',\n",
       " 'cost',\n",
       " 'effective',\n",
       " 'system']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_stop = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha ]\n",
    "tokens_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7abc0868-e1c8-4b75-b8d8-00edeceb80fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India | PROPN | NNP\n",
      "runs | VERB | VBZ\n",
      "on | ADP | IN\n",
      "open | ADJ | JJ\n",
      "source | NOUN | NN\n",
      ". | PUNCT | .\n",
      "From | ADP | IN\n",
      "small | ADJ | JJ\n",
      "startups | NOUN | NNS\n",
      "to | ADP | IN\n",
      "large | ADJ | JJ\n",
      "enterprises | NOUN | NNS\n",
      ", | PUNCT | ,\n",
      "developers | NOUN | NNS\n",
      "across | ADP | IN\n",
      "the | DET | DT\n",
      "country | NOUN | NN\n",
      "rely | VERB | VBP\n",
      "on | ADP | IN\n",
      "open | ADJ | JJ\n",
      "source | NOUN | NN\n",
      "tools | NOUN | NNS\n",
      "and | CCONJ | CC\n",
      "libraries | NOUN | NNS\n",
      "to | PART | TO\n",
      "build | VERB | VB\n",
      "fast | ADJ | JJ\n",
      ", | PUNCT | ,\n",
      "reliable | ADJ | JJ\n",
      ", | PUNCT | ,\n",
      "and | CCONJ | CC\n",
      "cost | NOUN | NN\n",
      "- | PUNCT | HYPH\n",
      "effective | ADJ | JJ\n",
      "systems | NOUN | NNS\n",
      ". | PUNCT | .\n"
     ]
    }
   ],
   "source": [
    "for token in doc: \n",
    "    print(token.text, '|', token.pos_,'|',token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e08bf13-88bf-4f53-a063-e582a1ee3a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean token : ['india', 'run', 'open', 'source', 'small', 'startup', 'large', 'enterprise', 'developer', 'country', 'rely', 'open', 'source', 'tool', 'library', 'build', 'fast', 'reliable', 'cost', 'effective', 'system']\n"
     ]
    }
   ],
   "source": [
    "print(\"clean token :\", tokens_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6267a27b-dc94-4f84-b949-adf9ad287a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_doc = \" \".join(tokens_stop)\n",
    "final_doc = nlp(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff4d2068-97ad-4364-9060-151c8cc5e2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "india run open source small startup large enterprise developer country rely open source tool library build fast reliable cost effective system"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f789139-90c7-4a78-b464-d9a16a39afee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india | PROPN | NNP\n",
      "run | VERB | VBD\n",
      "open | ADJ | JJ\n",
      "source | NOUN | NN\n",
      "small | ADJ | JJ\n",
      "startup | NOUN | NN\n",
      "large | ADJ | JJ\n",
      "enterprise | NOUN | NN\n",
      "developer | NOUN | NN\n",
      "country | NOUN | NN\n",
      "rely | VERB | VBP\n",
      "open | ADJ | JJ\n",
      "source | NOUN | NN\n",
      "tool | NOUN | NN\n",
      "library | NOUN | NN\n",
      "build | VERB | VBP\n",
      "fast | ADV | RB\n",
      "reliable | ADJ | JJ\n",
      "cost | NOUN | NN\n",
      "effective | ADJ | JJ\n",
      "system | NOUN | NN\n"
     ]
    }
   ],
   "source": [
    "for token in final_doc:\n",
    "    print(token.text, '|', token.pos_, '|', token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5fc58f-2495-47ee-9d20-b025ac275256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer , TfidfVectorizer\n\u001b[32m      3\u001b[39m vectorizer = CountVectorizer()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_counts = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_doc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/sklearn/feature_extraction/text.py:1377\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1369\u001b[39m             warnings.warn(\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1374\u001b[39m             )\n\u001b[32m   1375\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1380\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/sklearn/feature_extraction/text.py:1264\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1263\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/sklearn/feature_extraction/text.py:104\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         doc = \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m         doc = tokenizer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/sklearn/feature_extraction/text.py:62\u001b[39m, in \u001b[36m_preprocess\u001b[39m\u001b[34m(doc, accent_function, lower)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mapply to a document.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m    preprocessed string\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     doc = \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     doc = accent_function(doc)\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(final_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f3edad0-a5cf-4772-be5d-5c173f5358f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = \" \".join([token.lemma_ for token in final_doc if token.is_alpha and not token.is_stop])\n",
    "final_doc_list = [cleaned_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5c029a1-b14c-4e25-962e-dc272aac23be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india run open source small startup large enterprise developer country rely open source tool library build fast reliable cost effective system']\n"
     ]
    }
   ],
   "source": [
    "print(final_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba377a16-f6eb-47f7-add7-3146a3fb76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(final_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d296ddf-1766-42af-8703-4df3164f853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Vocabulary: ['build' 'cost' 'country' 'developer' 'effective' 'enterprise' 'fast'\n",
      " 'india' 'large' 'library' 'open' 'reliable' 'rely' 'run' 'small' 'source'\n",
      " 'startup' 'system' 'tool']\n",
      "Bag of Words matrix:\n",
      " [[1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words matrix:\\n\", X_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf0ecc66-746f-464d-9687-619e6b0e9d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary: ['build' 'cost' 'country' 'developer' 'effective' 'enterprise' 'fast'\n",
      " 'india' 'large' 'library' 'open' 'reliable' 'rely' 'run' 'small' 'source'\n",
      " 'startup' 'system' 'tool']\n",
      "TF-IDF matrix:\n",
      " [[0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.4 0.2 0.2\n",
      "  0.2]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(final_doc_list)\n",
    "print(\"TF-IDF Vocabulary:\", tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF matrix:\\n\", X_tfidf.toarray().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b1647de-dad4-4ec7-ba84-0928021333d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cca5dcf1-5f20-4ee0-b217-22265891a87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56baa469-f428-401b-ba98-b8f1a4d26ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.column = ['v1', 'v2']\n",
    "df['v1'] = df['v1'].map({'ham': 0, 'spam': 1})\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "x = df['v2']\n",
    "y = df['v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe0a2e58-512e-4eaf-88a9-169f8eddd15c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m.strip().map({\u001b[33m'\u001b[39m\u001b[33mham\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mspam\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/pandas/core/generic.py:6318\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6312\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6313\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6314\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6315\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6316\u001b[39m ):\n\u001b[32m   6317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/pandas/core/accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/pandas/core/strings/accessor.py:194\u001b[39m, in \u001b[36mStringMethods.__init__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstring_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28mself\u001b[39m._inferred_dtype = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_categorical = \u001b[38;5;28misinstance\u001b[39m(data.dtype, CategoricalDtype)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_string = \u001b[38;5;28misinstance\u001b[39m(data.dtype, StringDtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/citnlp/lib64/python3.13/site-packages/pandas/core/strings/accessor.py:248\u001b[39m, in \u001b[36mStringMethods._validate\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    245\u001b[39m inferred_dtype = lib.infer_dtype(values, skipna=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only use .str accessor with string values!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[31mAttributeError\u001b[39m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].str.strip().map({'ham': 0, 'spam': 1})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
